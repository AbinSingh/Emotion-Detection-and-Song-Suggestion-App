{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow the below link to download the dataset\n",
    "https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\Abin\\\\Desktop\\\\Emotion_Prediction') ## Replace this with your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.txt', header =None, sep =';', names = ['Input','Sentiment'], encoding='utf-8')\n",
    "df_test = pd.read_csv('test.txt', header = None, sep =';', names = ['Input','Sentiment'],encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Input Sentiment\n",
       "15995  i just had a very brief time in the beanbag an...   sadness\n",
       "15996  i am now turning and i feel pathetic that i am...   sadness\n",
       "15997                     i feel strong and good overall       joy\n",
       "15998  i feel like this was such a rude comment and i...     anger\n",
       "15999  i know a lot but i feel so stupid because i ca...   sadness"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                         Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. Data Sourcing, Data Cleaning and Sampling\n",
    "02. Bag of Words Model\n",
    "03. Cost Sensitive Learning and Evaluation Metrics\n",
    "04. Building Base-Line Model (Logistic Regression) and save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                           01. Data Sourcing, Data Cleaning and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         5362\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Sentiment.value_counts() # Imbalanced Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['format']='train'\n",
    "df_test['format']='test'\n",
    "\n",
    "total_df=pd.concat([df_train,df_test],axis=0)\n",
    "total_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                 01. Data Cleaning\n",
    "                                                  \n",
    "                                                  a. Tokenization (Word and Regex)\n",
    "                                                  b. Convert Tokens to Lower Case\n",
    "                                                  b. Filter Out Punctuation\n",
    "                                                  c. Filter out Stop Words (and Pipeline)\n",
    "                                                  d. Stem Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            a. Tokenization (word_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [i, didnt, feel, humiliated]\n",
       "1    [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2    [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3    [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4                            [i, am, feeling, grouchy]\n",
       "Name: tokenized_Tweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=total_df.copy()\n",
    "df['tokenized_Tweet'] = df.Input.values\n",
    "# Split each sentence into tokens\n",
    "df['tokenized_Tweet']=df['tokenized_Tweet'].apply(lambda row: nltk.word_tokenize(row))\n",
    "df['tokenized_Tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_Tweet']=df['tokenized_Tweet'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                           a. Tokenization (RegexpTokenizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else: \n",
    "(example: azithromycin-induced, we can drop hypen here. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [i, didnt, feel, humiliated]\n",
       "1    [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2    [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3    [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4                            [i, am, feeling, grouchy]\n",
       "Name: tokenized_Tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokenized_Tweet'] = df['tokenized_Tweet'].apply(lambda row: tokenizer.tokenize(row))\n",
    "df['tokenized_Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                               b. Convert Tokens to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [i, didnt, feel, humiliated]\n",
       "1    [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2    [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3    [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4                            [i, am, feeling, grouchy]\n",
       "Name: tokenized_Tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_Tweet'] = df['tokenized_Tweet'].apply(lambda row:[x.lower() for x in row])\n",
    "df['tokenized_Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                 C. Filter Out Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [i, didnt, feel, humiliated]\n",
       "1    [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2    [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3    [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4                            [i, am, feeling, grouchy]\n",
       "Name: token_filter, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_filter']=df['tokenized_Tweet']\n",
    "# remove all tokens that are not alphabetic\n",
    "df['token_filter'] = df['token_filter'].apply(lambda row: [x for x in row if x.isalpha()])\n",
    "df['token_filter'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                             d. Filter out Stop Words (and Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [didnt, feel, humiliated]\n",
       "1    [go, feeling, hopeless, damned, hopeful, aroun...\n",
       "2    [im, grabbing, minute, post, feel, greedy, wrong]\n",
       "3    [ever, feeling, nostalgic, fireplace, know, st...\n",
       "4                                   [feeling, grouchy]\n",
       "Name: token_filter_Stop, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['token_filter_Stop'] = df['token_filter'].apply(lambda row: [x for x in row if not x in stop_words])\n",
    "df['token_filter_Stop'].head() # We, the, about etc words are removed from the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                 e. Stem Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                [didnt, feel, humili]\n",
       "1    [go, feel, hopeless, damn, hope, around, someo...\n",
       "2         [im, grab, minut, post, feel, greedi, wrong]\n",
       "3    [ever, feel, nostalg, fireplac, know, still, p...\n",
       "4                                      [feel, grouchi]\n",
       "Name: token_filter_Stop_Stem, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_filter_Stop_Stem'] = df['token_filter_Stop'].apply(lambda row: [porter.stem(word) for word in row])\n",
    "df['token_filter_Stop_Stem'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              Sample to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy() # Copying the original data frame\n",
    "\n",
    "train_x=data.loc[data['format']=='train','token_filter_Stop_Stem']\n",
    "train_x=train_x.astype('str')\n",
    "y_train=data.loc[data['format']=='train','Sentiment']\n",
    "\n",
    "test_x=data.loc[data['format']=='test','token_filter_Stop_Stem']\n",
    "test_x=test_x.astype('str')\n",
    "y_test=data.loc[data['format']=='test','Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = { 'anger': 0,\n",
    "    'fear': 1,\n",
    "    'joy': 2,\n",
    "    'love': 3,\n",
    "    'sadness': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "train_y = [encoding[key] for key in y_train]\n",
    "test_y = [encoding[key] for key in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                  02. Bag of Words Model\n",
    "                              represent a whole sequence of words as a single feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16000x10357 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 143599 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()   # Instantiating and creating Count Vector object\n",
    "vectorizer.fit(train_x)\n",
    "X_train = vectorizer.transform(train_x)\n",
    "X_test  = vectorizer.transform(test_x)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save the model for future purpose\n",
    "joblib.dump(vectorizer,'vectorizer.pkl') ## helpful to tranform the future data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        03. Building Base-Line Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some required modules and libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import auc, make_scorer, recall_score, f1_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(class_weight='balanced')\n",
    "classifier.fit(X_train,train_y)\n",
    "score = classifier.score(X_test,test_y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Log_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save the model for future purpose\n",
    "joblib.dump(classifier,'Log_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
